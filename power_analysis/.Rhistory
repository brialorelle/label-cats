scale_color_brewer(palette="Set1") +
scale_x_continuous(limits=c(0, X_MAX), breaks=seq(0, X_MAX, 500))+
scale_y_continuous(limits=c(0, Y_MAX), breaks=seq(0, Y_MAX, 500))+
theme_bw() +
theme(legend.position = "none", axis.title.x = element_blank(),
axis.title.y = element_blank())
ggsave(paste0("processed_data/calib_adjust/",
"/",s,".pdf"), width=8, height=4)
}
d.cleaned <- adjusted_data
## between subjects variables not currently in pilot dataset: AuditoryCond,
##  data$stimGroup # stimuli grouping, between subjects variable
## data$AuditoryCond
d.cleaned$AuditoryCond ="Labels"
d.cleaned$StimGroup = "StimGroup1"
d.cleaned$SimLevel = d$level
# Rename variables for clarity
d.cleaned$SimLevelDiscrete = d.cleaned$level
# d.cleaned$SimLevel = d.cleaned$similarity: will be pearson's correlation values between stimulus categories
d.cleaned$Participant = d.cleaned$subid
# 1. Filter data for word learning and test trials
toFilter=c('WordLearn','TestTrial')
d.cleaned <- d.cleaned %>%
filter(is.element(TrialType,toFilter))
# 2. Define the target ROIs (regions of interest)
rois <- list()
rois[[1]] <- c(0,200,768,600) # left
rois[[2]] <- c(1152,200,768,600) # right
names(rois) <- c("L","R")
roi.image(rois)
d.cleaned$roi <- roi.check(d.cleaned,rois) # calls helper function to say whether infant was looking at ROI or not
# 3. Code whether infant was looking in each ROI
d.cleaned <- d.cleaned %>%
mutate(novelty = ifelse(roi == distPos, "1", "0")) %>%
mutate(novelty = as.numeric(novelty))
# see how the distribution of ROIs looks
qplot(roi,data=d.cleaned) # mostly looking in one or the other
# Rezero trials to onset of stimuli (test trial) or to onset of label (word learning trials)
d.cleaned <- rezero.trials(d.cleaned )
roi.image(rois)
d$x
plot(d$x,d$y)
toFilter=c('WordLearn','TestTrial')
d.cleaned <- d.cleaned %>%
filter(is.element(TrialType,toFilter))
d.cleaned$TrialType
rois <- list()
rois[[1]] <- c(0,200,768,600) # left
rois[[2]] <- c(1152,200,768,600) # right
names(rois) <- c("L","R")
roi.image(rois)
d.cleaned$roi <- roi.check(d.cleaned,rois) # calls helper function to say whether infant was looking at ROI or not
d.cleaned <- d.cleaned %>%
mutate(novelty = ifelse(roi == distPos, "1", "0")) %>%
mutate(novelty = as.numeric(novelty))
qplot(roi,data=d.cleaned) # mostly looking in one or the other
plot(d.cleaned$x,d.cleaned$y)
qplot(x,y,
facets = ~subid,
geom="point",
data=subset(d,TrialType == "TestTrial"),
xlim=c(0,1920),
ylim=c(0,1080))
qplot(x,y,
facets = ~subid,
geom="point",
data=subset(d,TrialType == "WordLearn"),
xlim=c(0,1920),
ylim=c(0,1080))
qplot(x,y,
facets = ~subid,
geom="density2d",
data=subset(d,TrialType == "TestTrial"),
xlim=c(0,1920),
ylim=c(0,1080))
qplot(x,y,
facets = ~subid,
geom="point",
data=subset(d,TrialType == "WordLearn"),
xlim=c(0,1920),
ylim=c(0,1080))
qplot(x,y,
facets = ~subid,
geom="density2d",
data=subset(d,TrialType == "Calibration"),
xlim=c(0,1920),
ylim=c(0,1080))
qplot(x,y,
facets = ~subid,
geom="density2d",
data=subset(d,TrialType == "TestTrial"),
xlim=c(0,1920),
ylim=c(0,1080))
# 1. Filter data for word learning and test trials
toFilter=c('WordLearn','TestTrial')
d.cleaned <- d.cleaned %>%
filter(is.element(TrialType,toFilter))
# 2. Define the target ROIs (regions of interest)
rois <- list()
rois[[1]] <- c(0,200,768,600) # left
rois[[2]] <- c(1152,200,768,600) # right
names(rois) <- c("L","R")
roi.image(rois)
d.cleaned$roi <- roi.check(d.cleaned,rois) # calls helper function to say whether infant was looking at ROI or not
# 3. Code whether infant was looking in each ROI
d.cleaned <- d.cleaned %>%
mutate(novelty = ifelse(roi == distPos, "1", "0")) %>%
mutate(novelty = as.numeric(novelty))
# see how the distribution of ROIs looks
qplot(roi,data=d.cleaned) # mostly looking in one or the other
# Rezero trials to onset of stimuli (test trial) or to onset of label (word learning trials)
d.cleaned <- rezero.trials(d.cleaned )
LabelsNovelty = novPrefsByCond$meanNovelty[novPrefsByCond$AuditoryCond=="Labels"]
LabelsNovelty
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(BayesFactor)
library(BFDA)
meta=as.tibble(read.csv('Label advantage in concept learning.csv'))
# reported in abstract, across all conditions and ages
effectSizes_All<- meta %>%
filter(audio_condition %in% c("noun","silence", "non-linguistic sound")) %>%
group_by(audio_condition) %>%
summarize(meanEffect = mean(g_calc), sdEffect = sd(d_calc))%>%
# used for power analysis -- target
effectSizes_Targeted<- meta %>%
filter(audio_condition %in% c("noun","silence", "non-linguistic sound")) %>%
filter(mean_age_months > 12 & mean_age_months <16) %>%
filter(response_mode=="eye-tracking") %>%
group_by(audio_condition) %>%
summarize(meanEffect_Targeted = mean(g_calc),sdEffect_Targeted = sd(g_calc))
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(BayesFactor)
library(BFDA)
meta=as.tibble(read.csv('Label advantage in concept learning.csv'))
# reported in abstract, across all conditions and ages
effectSizes_All<- meta %>%
filter(audio_condition %in% c("noun","silence", "non-linguistic sound")) %>%
group_by(audio_condition) %>%
summarize(meanEffect = mean(g_calc), sdEffect = sd(d_calc))%>%
# used for power analysis -- target
effectSizes_Targeted<- meta %>%
filter(audio_condition %in% c("noun","silence", "non-linguistic sound")) %>%
filter(mean_age_months > 12 & mean_age_months <16) %>%
filter(response_mode=="eye-tracking") %>%
group_by(audio_condition) %>%
summarize(meanEffect_Targeted = mean(g_calc),sdEffect_Targeted = sd(g_calc))
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(BayesFactor)
library(BFDA)
meta=as.tibble(read.csv('Label advantage in concept learning.csv'))
# reported in abstract, across all conditions and ages
effectSizes_All<- meta %>%
filter(audio_condition %in% c("noun","silence", "non-linguistic sound")) %>%
group_by(audio_condition) %>%
summarize(meanEffect = mean(g_calc), sdEffect = sd(d_calc))%>%
meta=as.tibble(read.csv('Label advantage in concept learning.csv'))
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(BayesFactor)
library(BFDA)
setwd("~/Documents/GitHub/label-cats/power_analysis")
meta=as.tibble(read.csv('Label advantage in concept learning.csv'))
# reported in abstract, across all conditions and ages
effectSizes_All<- meta %>%
filter(audio_condition %in% c("noun","silence", "non-linguistic sound")) %>%
group_by(audio_condition) %>%
summarize(meanEffect = mean(g_calc), sdEffect = sd(d_calc))
# used for power analysis -- target
effectSizes_Targeted<- meta %>%
filter(audio_condition %in% c("noun","silence", "non-linguistic sound")) %>%
filter(mean_age_months > 12 & mean_age_months <16) %>%
filter(response_mode=="eye-tracking") %>%
group_by(audio_condition) %>%
summarize(meanEffect_Targeted = mean(g_calc),sdEffect_Targeted = sd(g_calc))
knitr::kable(effectSizes_All, digits = 2)
knitr::kable(effectSizes_Targeted, digits = 2)
## parameters for both simulations and analyses
boundaryBF = 10
n.min = 20
n.max = 80
stepsize = 10
## H1: Difference between labels and tones is as predicted by meta-analysis
LabelsVTones = effectSizes_Targeted$meanEffect_Targeted[2]-effectSizes_Targeted$meanEffect_Targeted[1]
s1 <- BFDA.sim(expected.ES=LabelsVTones, n.min=n.min, stepsize=stepsize, n.max=n.max, type="t.between", design="sequential", r=sqrt(2)/2, alternative="directional", cores=4, boundary=boundaryBF)
a1 <- BFDA.analyze(s1, design="sequential", n.min=n.min, boundary=boundaryBF)
plot(s1)
## Null hypothesis: no difference between labels and tones
s0 <- BFDA.sim(expected.ES=0, n.min=n.min, stepsize=stepsize, n.max=n.max, type="t.between", design="sequential", r=sqrt(2)/2, alternative="directional", cores=4,boundary=boundaryBF)
a0 <- BFDA.analyze(s0, design="sequential", n.min=n.min, boundary=boundaryBF)
plot(s0)
LabelsVTones
LabelsVTones = effectSizes_Targeted$meanEffect_Targeted[2]-effectSizes_Targeted$meanEffect_Targeted[1]
s1 <- BFDA.sim(expected.ES=LabelsVTones, n.min=n.min, stepsize=stepsize, n.max=n.max, type="t.between", design="sequential", r=sqrt(2)/2, alternative="directional", cores=4, boundary=boundaryBF)
a1 <- BFDA.analyze(s1, design="sequential", n.min=n.min, boundary=boundaryBF)
plot(s1)
s1
LabelsVTones
a1
## parameters for both simulations and analyses
boundaryBF = 10
n.min = 20
n.max = 50
stepsize = 5
## H1: Difference between labels and tones is as predicted by meta-analysis
LabelsVTones = effectSizes_Targeted$meanEffect_Targeted[2]-effectSizes_Targeted$meanEffect_Targeted[1]
s1 <- BFDA.sim(expected.ES=LabelsVTones, n.min=n.min, stepsize=stepsize, n.max=n.max, type="t.between", design="sequential", r=sqrt(2)/2, alternative="directional", cores=4, boundary=boundaryBF)
a1 <- BFDA.analyze(s1, design="sequential", n.min=n.min, boundary=boundaryBF)
plot(s1)
## Null hypothesis: no difference between labels and tones
s0 <- BFDA.sim(expected.ES=0, n.min=n.min, stepsize=stepsize, n.max=n.max, type="t.between", design="sequential", r=sqrt(2)/2, alternative="directional", cores=4,boundary=boundaryBF)
a0 <- BFDA.analyze(s0, design="sequential", n.min=n.min, boundary=boundaryBF)
plot(s0)
s0
s1
plot(s1)
warnings()
LabelsVTones
effectSizes_Targeted
effectSizes_Targeted<- meta %>%
filter(audio_condition %in% c("noun","silence", "non-linguistic sound")) %>%
filter(mean_age_months > 12 & mean_age_months <16) %>%
filter(response_mode=="eye-tracking") %>%
group_by(audio_condition)
effectSizes_Targeted
effectSizes_Targeted$study_ID
effectSizes_Targeted$g_calc
effectSizes_Targeted$g_calc[effectSizes_Targeted$audio_condition=="silence"]
sd(effectSizes_Targeted$g_calc[effectSizes_Targeted$audio_condition=="silence"])
sd(effectSizes_Targeted$g_calc[effectSizes_Targeted$audio_condition=="non-linguistic tones"])
effectSizes_Targeted$g_calc[effectSizes_Targeted$audio_condition=="non-linguistic tones"]
effectSizes_Targeted$g_calc[effectSizes_Targeted$audio_condition=="non-linguistic tones"]
effectSizes_Targeted$g_calc[effectSizes_Targeted$audio_condition=="non-linguistic tones"]
effectSizes_Targeted$g_calc[effectSizes_Targeted$audio_condition=="non-linguistic sound"]
effectSizes_Targeted$g_calc[effectSizes_Targeted$audio_condition=="non-linguistic sound"]
meta
meta$g_calc
meta$g_calc[meta$audio_condition=="non-linguistic sound"]
meta$age[meta$audio_condition=="non-linguistic sound"]
meta$age_moths[meta$audio_condition=="non-linguistic sound"]
meta$mean_age_months[meta$audio_condition=="non-linguistic sound"]
effectSizes_Targeted<- meta %>%
filter(audio_condition %in% c("noun","silence", "non-linguistic sound")) %>%
filter(mean_age_months >= 12) %>%
filter(mean_age_months <= 15) %>%
filter(response_mode=="eye-tracking") %>%
group_by(audio_condition) %>%
summarize(meanEffect_Targeted = mean(g_calc),sdEffect_Targeted = sd(g_calc))
effectSizes_Targeted
effectSizes_Targeted<- meta %>%
filter(audio_condition %in% c("noun","silence", "non-linguistic sound")) %>%
filter(mean_age_months >= 12) %>%
filter(mean_age_months <= 16) %>%
filter(response_mode=="eye-tracking") %>%
group_by(audio_condition) %>%
summarize(meanEffect_Targeted = mean(g_calc),sdEffect_Targeted = sd(g_calc))
effectSizes_Targeted
effectSizes_Targeted<- meta %>%
filter(audio_condition %in% c("noun","silence", "non-linguistic sound")) %>%
filter(mean_age_months >= 12) %>%
filter(mean_age_months <= 16) %>%
filter(response_mode=="eye-tracking") %>%
group_by(audio_condition)
effectSizes_Targeted
effectSizes_Targeted$g_calc
effectSizes_Targeted<- meta %>%
filter(audio_condition %in% c("noun","silence", "non-linguistic sound")) %>%
filter(mean_age_months >= 12) %>%
filter(mean_age_months <= 16) %>%
filter(response_mode=="eye-tracking") %>%
group_by(audio_condition) %>%
summarize(meanEffect_Targeted = mean(g_calc),sdEffect_Targeted = sd(g_calc))
effectSizes_Targeted
effectSizes_Targeted<- meta %>%
filter(audio_condition %in% c("noun","silence", "non-linguistic sound")) %>%
filter(mean_age_months >= 12) %>%
filter(mean_age_months <= 16) %>%
filter(response_mode=="eye-tracking") %>%
group_by(audio_condition)
effectSizes_Targeted<- meta %>%
filter(audio_condition %in% c("noun","silence", "non-linguistic sound")) %>%
filter(mean_age_months >= 11) %>%
filter(mean_age_months <= 17) %>%
filter(response_mode=="eye-tracking") %>%
group_by(audio_condition)
effectSizes_Targeted<- meta %>%
filter(audio_condition %in% c("noun","silence", "non-linguistic sound")) %>%
filter(mean_age_months >= 11) %>%
filter(mean_age_months <= 17) %>%
filter(response_mode=="eye-tracking") %>%
group_by(audio_condition)
effectSizes_Targeted
effectSizes_Targeted<- meta %>%
filter(audio_condition %in% c("noun","silence", "non-linguistic sound")) %>%
filter(mean_age_months >= 11) %>%
filter(mean_age_months <= 17) %>%
filter(response_mode=="eye-tracking") %>%
group_by(audio_condition) %>%
summarize(meanEffect_Targeted = mean(g_calc),sdEffect_Targeted = sd(g_calc))
effectSizes_Targeted
effectSizes_Targeted<- meta %>%
filter(audio_condition %in% c("noun","silence", "non-linguistic sound")) %>%
filter(mean_age_months >= 11) %>%
filter(mean_age_months <= 17) %>%
filter(response_mode=="eye-tracking") %>%
group_by(audio_condition) %>%
summarize(meanEffect_Targeted = mean(g_calc),sdEffect_Targeted = sd(g_calc), countStudies = sum(g_calc>0))
effectSizes_Targeted
effectSizes_Targeted<- meta %>%
filter(audio_condition %in% c("noun","silence", "non-linguistic sound")) %>%
filter(mean_age_months >= 11) %>%
filter(mean_age_months <= 17) %>%
filter(response_mode=="eye-tracking") %>%
group_by(audio_condition) %>%
summarize(meanEffect_Targeted = mean(g_calc),sdEffect_Targeted = sd(g_calc), countStudies = count_g_calc)
effectSizes_Targeted<- meta %>%
filter(audio_condition %in% c("noun","silence", "non-linguistic sound")) %>%
filter(mean_age_months >= 11) %>%
filter(mean_age_months <= 17) %>%
filter(response_mode=="eye-tracking") %>%
group_by(audio_condition) %>%
summarize(meanEffect_Targeted = mean(g_calc),sdEffect_Targeted = sd(g_calc), countStudies = count(g_calc))
effectSizes_Targeted<- meta %>%
filter(audio_condition %in% c("noun","silence", "non-linguistic sound")) %>%
filter(mean_age_months >= 11) %>%
filter(mean_age_months <= 17) %>%
filter(response_mode=="eye-tracking") %>%
group_by(audio_condition) %>%
summarize(meanEffect_Targeted = mean(g_calc),sdEffect_Targeted = sd(g_calc), countStudies = length(g_calc))
effectSizes_Targeted
s0
a0
## parameters for both simulations and analyses
boundaryBF = 10
n.min = 20
n.max = 50
stepsize = 5
## H1: Difference between labels and tones is as predicted by meta-analysis
LabelsVTones = effectSizes_Targeted$meanEffect_Targeted[2]-effectSizes_Targeted$meanEffect_Targeted[1]
s1 <- BFDA.sim(expected.ES=LabelsVTones, n.min=n.min, stepsize=stepsize, n.max=n.max, type="t.between", design="sequential", r=sqrt(2)/2, alternative="directional", cores=4, boundary=boundaryBF)
a1 <- BFDA.analyze(s1, design="sequential", n.min=n.min, boundary=boundaryBF)
plot(s1)
## Null hypothesis: no difference between labels and tones
s0 <- BFDA.sim(expected.ES=0, n.min=n.min, stepsize=stepsize, n.max=n.max, type="t.between", design="sequential", r=sqrt(2)/2, alternative="directional", cores=4,boundary=boundaryBF)
a0 <- BFDA.analyze(s0, design="sequential", n.min=n.min, boundary=boundaryBF)
plot(s0)
LabelsVTones
effectSizes_All
meta=as.tibble(read.csv('Label advantage in concept learning.csv'))
# reported in abstract, across all conditions and ages
effectSizes_All<- meta %>%
filter(audio_condition %in% c("noun","silence", "non-linguistic sound")) %>%
group_by(audio_condition) %>%
summarize(meanEffect = mean(g_calc), sdEffect = sd(d_calc), countStudies = length(g_calc))
# used for power analysis -- target
effectSizes_Targeted<- meta %>%
filter(audio_condition %in% c("noun","silence", "non-linguistic sound")) %>%
filter(mean_age_months >= 11) %>%
filter(mean_age_months <= 17) %>%
filter(response_mods0e=="eye-tracking") %>%
group_by(audio_condition) %>%
summarize(meanEffect_Targeted = mean(g_calc),sdEffect_Targeted = sd(g_calc), countStudies = length(g_calc))
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(BayesFactor)
library(BFDA)
meta=as.tibble(read.csv('Label advantage in concept learning.csv'))
# reported in abstract, across all conditions and ages
effectSizes_All<- meta %>%
filter(audio_condition %in% c("noun","silence", "non-linguistic sound")) %>%
group_by(audio_condition) %>%
summarize(meanEffect = mean(g_calc), sdEffect = sd(d_calc), countStudies = length(g_calc))
# used for power analysis -- target
effectSizes_Targeted<- meta %>%
filter(audio_condition %in% c("noun","silence", "non-linguistic sound")) %>%
filter(mean_age_months >= 11) %>%
filter(mean_age_months <= 17) %>%
filter(response_modse=="eye-tracking") %>%
group_by(audio_condition) %>%
summarize(meanEffect_Targeted = mean(g_calc),sdEffect_Targeted = sd(g_calc), countStudies = length(g_calc))
LabelsVTones
LabelsVTones = effectSizes_Targeted$meanEffect_Targeted[2]-effectSizes_Targeted$meanEffect_Targeted[1]
LabelsVTones
effectSizes_Targeted
effectSizes_All
a0
s0 <- BFDA.sim(expected.ES=0, n.min=n.min, stepsize=stepsize, n.max=n.max, type="t.between", design="sequential", r=sqrt(2)/2, alternative="directional", cores=4,boundary=boundaryBF)
a0 <- BFDA.analyze(s0, design="sequential", n.min=n.min, boundary=boundaryBF)
plot(s0)
a0
LabelsVTones
effectSizes_Targeted<- meta %>%
filter(audio_condition %in% c("noun","silence", "non-linguistic sound")) %>%
filter(mean_age_months >= 11) %>%
filter(mean_age_months <= 17) %>%
group_by(audio_condition, response_mode) %>%
summarize(meanEffect_Targeted = mean(g_calc),sdEffect_Targeted = sd(g_calc), countStudies = length(g_calc))
effectSizes_Targeted
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(BayesFactor)
library(BFDA)
meta=as.tibble(read.csv('Label advantage in concept learning.csv'))
# reported in abstract, across all conditions and ages
effectSizes_All<- meta %>%
filter(audio_condition %in% c("noun","silence", "non-linguistic sound")) %>%
group_by(audio_condition) %>%
summarize(meanEffect = mean(g_calc), sdEffect = sd(d_calc), countStudies = length(g_calc))
# used for power analysis -- target sample sizes
effectSizes_Targeted<- meta %>%
filter(audio_condition %in% c("noun","silence", "non-linguistic sound")) %>%
filter(mean_age_months >= 11) %>%  # using 11 rather than 12 allows to include a few extra studeis
group_by(audio_condition, response_mode) %>%
summarize(meanEffect_Targeted = mean(g_calc), sdEffect_Targeted = sd(g_calc), countStudies = length(g_calc))
LabelsVTones = effectSizes_Targeted$meanEffect_Targeted[2]-effectSizes_Targeted$meanEffect_Targeted[1]
LabelsVTones
meta=as.tibble(read.csv('Label advantage in concept learning.csv'))
# reported in abstract, across all conditions and ages
effectSizes_All<- meta %>%
filter(audio_condition %in% c("noun","silence", "non-linguistic sound")) %>%
group_by(audio_condition) %>%
summarize(meanEffect = mean(g_calc), sdEffect = sd(d_calc), countStudies = length(g_calc))
# used for power analysis -- target sample sizes
effectSizes_Targeted<- meta %>%
filter(audio_condition %in% c("noun","silence", "non-linguistic sound")) %>%
filter(mean_age_months > 11) %>%  # using 11 rather than 12 allows to include a few extra studeis
filter(response_mode ==  "eye-tracking") %>%
group_by(audio_condition) %>%
summarize(meanEffect_Targeted = mean(g_calc), sdEffect_Targeted = sd(g_calc), countStudies = length(g_calc))
LabelsVTones = effectSizes_Targeted$meanEffect_Targeted[2]-effectSizes_Targeted$meanEffect_Targeted[1]
LabelsVTones
round(LabelsVTones,2)
## parameters for both simulations and analyses
boundaryBF = 10
n.min = 20
n.max = 50
stepsize = 5
## H1: Difference between labels and tones is as predicted by meta-analysis
LabelsVTones = effectSizes_Targeted$meanEffect_Targeted[2]-effectSizes_Targeted$meanEffect_Targeted[1]
s1 <- BFDA.sim(expected.ES=LabelsVTones, n.min=n.min, stepsize=stepsize, n.max=n.max, type="t.between", design="sequential", r=sqrt(2)/2, alternative="directional", cores=4, boundary=boundaryBF)
a1 <- BFDA.analyze(s1, design="sequential", n.min=n.min, boundary=boundaryBF)
plot(s1)
## Null hypothesis: no difference between labels and tones
s0 <- BFDA.sim(expected.ES=0, n.min=n.min, stepsize=stepsize, n.max=n.max, type="t.between", design="sequential", r=sqrt(2)/2, alternative="directional", cores=4,boundary=boundaryBF)
a0 <- BFDA.analyze(s0, design="sequential", n.min=n.min, boundary=boundaryBF)
plot(s0)
## Null hypothesis: no difference between labels and tones
s0 <- BFDA.sim(expected.ES=0, n.min=n.min, stepsize=stepsize, n.max=n.max, type="t.between", design="sequential", r=sqrt(2)/2, alternative="directional", cores=4,boundary=boundaryBF)
a0 <- BFDA.analyze(s0, design="sequential", n.min=n.min, boundary=boundaryBF)
plot(s0)
knitr::kable(a0, digits = 2)
a0
data.frame(a0)
a0$settings
knitr::opts_chunk$set(echo = TRUE)
suppressPackageStartupMessages(c("dplyr","langcog","tidyr","ggplot2","lme4"))
library(dplyr)
library(ggplot2)
library(rjson)
library(stringr)
library(tidyr)
library(dplyr)
library(forcats)
library(langcog)
files <- dir("../production-results/")
d.raw <- data.frame()
for (f in files) {
jf <- paste("../production-results/",f,sep="")
jd <- fromJSON(paste(readLines(jf), collapse=""))
id <- data.frame(workerid = jd$WorkerId,
seenObject = as.numeric(jd$answers$data$seenObject)[1:85],
knowFunction = as.numeric(jd$answers$data$knowFunction)[1:85],
knowLabel = as.numeric(jd$answers$data$knowObjectLabel)[1:85],
childsAge = jd$answers$data$childsAge,
imageName = jd$answers$data$imageName)
d.raw <- bind_rows(d.raw, id)
}
# function for shorter filename extraction
shortFileName <- function(fileName){
out=strsplit(as.character(fileName),"/")[[1]][8]
}
# make shorter iamge names for plots, etc.
d.raw <- d.raw %>%
group_by(imageName) %>%
mutate(imageNameShort = shortFileName(imageName))
files
f
d.raw
# pre-process: get children's age and filter by too young/old
thres=.25
d.unfamiliar <- d.raw %>%
group_by(imageNameShort) %>%
summarize(avgSeenObject = mean(seenObject), avgKnowLabel = mean(knowLabel), avgKnowFunction =     mean(knowFunction)) %>%
mutate(unFamiliar = (avgSeenObject<= thres & avgKnowLabel<=thres  & avgKnowFunction<=thres)) %>%
filter(unFamiliar == TRUE) %>% ## only get unfamiliar objects
mutate(imageNameShort = fct_reorder(imageNameShort, avgSeenObject)) ## reorders varibles accordinf to #avgSeenObject for plotting
ggplot(d.unfamiliar, aes(x = as.factor(imageNameShort), y = avgSeenObject)) +
geom_point() +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
d.unfamiliar
d.unfamiliar$imageNameShort
summary(d.unfamiliar$imageNameShort)
dim(d.unfamiliar)
d
d.unfamiliar
d.unfamiliar
d.unfamiliar$imageNameShort
d.unfamiliar$imageNameShort=="dumbbell.jpg"
d.pretty$imageNameShort=="dumbbell.jpg"
d.unfamiliar$imageNameShort
