a0
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(BayesFactor)
library(BFDA)
## parameters for both simulations and analyses
boundaryBF = 10s0
## parameters for both simulations and analyses
boundaryBF = 10
n.min = 20
n.max = 80
stepsize = 10
## H1: Difference between labels and tones is as predicted by meta-analysis
LabelsVTones = effectSizes_Targeted$meanEffect_Targeted[2]-effectSizes_Targeted$meanEffect_Targeted[1]
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(BayesFactor)
library(BFDA)
meta=as.tibble(read.csv('Label advantage in concept learning.csv'))
effectSizes_All<- meta %>%
filter(audio_condition %in% c("noun","silence", "non-linguistic sound")) %>%
group_by(audio_condition) %>%
summarize(meanEffect = mean(g_calc), sdEffect = sd(d_calc))%>%
data.frame
effectSizes_Targeted<- meta %>%
filter(audio_condition %in% c("noun","silence", "non-linguistic sound")) %>%
mutate(mean_age_months > 12) %>%
filter(response_mode=="eye-tracking") %>%
group_by(audio_condition) %>%
summarize(meanEffect_Targeted = mean(g_calc),sdEffect_Targeted = sd(g_calc))
knitr::kable(effectSizes_All, digits = 2)
knitr::kable(effectSizes_All, digits = 2)
## parameters for both simulations and analyses
boundaryBF = 10
n.min = 20
n.max = 80
stepsize = 10
## H1: Difference between labels and tones is as predicted by meta-analysis
LabelsVTones = effectSizes_Targeted$meanEffect_Targeted[2]-effectSizes_Targeted$meanEffect_Targeted[1]
s1 <- BFDA.sim(expected.ES=LabelsVTones, n.min=n.min, stepsize=stepsize, n.max=n.max, type="t.between", design="sequential", r=sqrt(2)/2, alternative="directional", cores=4, boundary=boundaryBF)
a1 <- BFDA.analyze(s1, design="sequential", n.min=n.min, boundary=boundaryBF)
plot(s1)
## Null hypothesis: no difference between labels and tones
s0 <- BFDA.sim(expected.ES=0, n.min=n.min, stepsize=stepsize, n.max=n.max, type="t.between", design="sequential", r=sqrt(2)/2, alternative="directional", cores=4,boundary=boundaryBF)
a0 <- BFDA.analyze(s0, design="sequential", n.min=n.min, boundary=boundaryBF)
plot(s0)
s1
a1
a0
s0
a0
a1
knitr::opts_chunk$set(echo = TRUE)
library(here)
library(BayesFactor)
library(brms)
library(MASS)
library(tidyverse)
# Directories and helper code
path="analyses"
## Read in pre-processed data stream
d <- read.csv(here::here(path, "processed_data/labelcats-pilot-all.csv"))
setwd("~/Documents/GitHub/label-cats/analyses-pilot")
# Directories and helper code
path="analyses"
## Read in pre-processed data stream
d <- read.csv(here::here(path, "processed_data/labelcats-pilot-all.csv"))
knitr::opts_chunk$set(echo = TRUE)
library(here)
library(BayesFactor)
library(brms)
library(MASS)
library(tidyverse)
# Directories and helper code
path="analyses"
## Read in pre-processed data stream
d <- read.csv(here::here(path, "processed_data/labelcats-pilot-all.csv"))
path
# Directories and helper code
path="analyses-pilot"
## Read in pre-processed data stream
d <- read.csv(here::here(path, "processed_data/labelcats-pilot-all.csv"))
## Read in .csv sheet with the SMI names for the stimuli we're going to be looking at
condOrder <- read.csv(here::here(path,"info/stimInfo.csv"))
# Get sets of codes useful for importing SMI eyetracking data
source(here::here(path,"HelperCode/et_helper_bll.R"))
## Eliminate data points where the eye-tracked failed to get accurate gaze
# 1. only get data where L/R eyes are valid
# 2. then, make those x and y's that fell on (0, 1080) as weird default be treated as NA's.
X_MAX=1920
Y_MAX=1080
d <- as.tibble(d) %>%
mutate(x = replace(x, LeyeValid==0 | d$ReyeValid==0,NA), y = replace(y, LeyeValid==0 | d$ReyeValid==0,NA)) %>%
mutate(x = ifelse(x <= 0 | x >= X_MAX | y <= 0 | y >= Y_MAX, NA, x),
y = ifelse(x <= 0 | x >= X_MAX | y <= 0 | y >= Y_MAX, NA, y))
## Join condition info with eye-tracking info
d<- d %>%
inner_join(condOrder)
## Count how many test trials recorded for each participant
testTrialCount <- d %>%
group_by(subid, TrialType) %>%
filter(TrialType =="TestTrial") %>%
dplyr::summarise(testTrialCount = length(unique(stimulus)))
# Get rid of subjects who never made it to test trials
d <- d %>%
filter(is.element(subid,testTrialCount$subid))
# Count what proportion of the time kids were looking at test trials, compute include trial index
trialsToInclude <- d %>%
filter(TrialType %in% c("TestTrial","WordLearn","Calibration")) %>%
group_by(subid, StimName,TrialType) %>%
dplyr::summarise(samples = length(x), na_prop = sum(is.na(x)) / length(x)) %>%
mutate(includeTrial = (na_prop)<.5)
# Only keep subjects who have at least two silent test trials
subsToInclude <- trialsToInclude %>%
group_by(subid, TrialType) %>%
dplyr::summarize(countTrials = sum(includeTrial)) %>%
filter(TrialType == "TestTrial") %>%
mutate(includeSub = countTrials > 1) %>% # At least 2/3 test trials
filter(includeSub == TRUE)
# Now filter dataset
d <- d %>%
filter(is.element(subid,subsToInclude$subid)) %>% # only include participants wtih enough data
inner_join(trialsToInclude) ## only include trials with enough data
# check we filtered subjects
unique(d$subid)
# Look at recalibration trials
qplot(x,y,
facets = ~subid,
geom="density2d",
data=subset(d,TrialType == "Calibration"),
xlim=c(0,1920),
ylim=c(0,1080))
# Look at test trials
qplot(x,y,
facets = ~subid,
geom="density2d",
data=subset(d,TrialType == "TestTrial"),
xlim=c(0,1920),
ylim=c(0,1080))
## copied largely from github.com/dyurovsky/refword/loading_helpers/adjust_calibs
CALIB_SACCADE_TIME = .3 # time it should take for infants to saccade to the actual stimulus if they are following perfectly (300ms)
pts <- read_csv(here::here("analyses/info/calibPoints_example.csv")) %>%
filter(instance == 1)%>%
mutate(startTimeBegin = min (start_time)) %>%  ## csv has different kind of timestamps, converting
mutate(start_time = start_time - startTimeBegin + CALIB_SACCADE_TIME,
end_time = end_time - startTimeBegin + CALIB_SACCADE_TIME)
## copied largely from github.com/dyurovsky/refword/loading_helpers/adjust_calibs
CALIB_SACCADE_TIME = .3 # time it should take for infants to saccade to the actual stimulus if they are following perfectly (300ms)
pts <- read_csv(here::here("analyses/info/calibPoints_example.csv")) %>%
filter(instance == 1)%>%
mutate(startTimeBegin = min (start_time)) %>%  ## csv has different kind of timestamps, converting
mutate(start_time = start_time - startTimeBegin + CALIB_SACCADE_TIME,
end_time = end_time - startTimeBegin + CALIB_SACCADE_TIME)
## copied largely from github.com/dyurovsky/refword/loading_helpers/adjust_calibs
CALIB_SACCADE_TIME = .3 # time it should take for infants to saccade to the actual stimulus if they are following perfectly (300ms)
pts <- read_csv(here::here("analyses-pilot/info/calibPoints_example.csv")) %>%
filter(instance == 1)%>%
mutate(startTimeBegin = min (start_time)) %>%  ## csv has different kind of timestamps, converting
mutate(start_time = start_time - startTimeBegin + CALIB_SACCADE_TIME,
end_time = end_time - startTimeBegin + CALIB_SACCADE_TIME)
## get calibration data from first time
learn_data <- d %>%
filter(TrialType == "Calibration") %>%
filter(t < 100) %>% ## Only get first recalibration stimulus -- will have diff names in final version.
group_by(subid) %>%
mutate(Time = t.stim - min(t.stim))  %>%
filter(!is.na(y) & !is.na(x))
## filter calibration data by the timepoints when the ball was in certain locations
calib_data <- lapply(1:nrow(pts), function (pt) {
learn_data %>%
filter(Time >= pts[pt,]$start_time,
Time <= pts[pt,]$end_time) %>%
mutate(point = pts[pt,]$point,
instance = pts[pt,]$instance)}) %>%
bind_rows %>%
arrange(subid, Time) %>%
group_by(subid) %>%
left_join(pts)
## robust regression on true and predicted outcomes
x_models <- calib_data %>%
do(x_model = rlm(true_x ~ x, data = .))
y_models <- calib_data %>%
do(y_model = rlm(true_y ~ y, data = .))
models <- left_join(x_models,y_models) # bind x and y models
subjs <- unique(calib_data$subid) # get list of subjects
## use these robust regressions to predict new values for the recalibraiton trials
predicted_data <- lapply(subjs, function (s) {
models <- filter(models, subid == s)
filter(calib_data, subid == s) %>%
mutate(predicted_x = predict(models$x_model[[1]]),
predicted_y = predict(models$y_model[[1]]))}) %>%
bind_rows %>%
dplyr::rename(empirical_x = x, empirical_y = y) %>%
group_by(subid) %>%
gather(measure,value,empirical_x,predicted_x,empirical_y,predicted_y) %>%
separate(measure, into = c("measure", "dimension"), sep = "\\_") %>%
spread(dimension,value)
## plot individual adjusted calibrations for these recalibration trials
s="102017_pilot1-eye_data Samples.txt"
subj_data = filter(predicted_data,subid == s)
ggplot(aes(x = x,y = y,color=interaction(measure,instance)), data = subj_data) +
facet_grid(. ~ measure) +
geom_point(size = .8) +
geom_point(aes(x = true_x, y = true_y), color="black", shape = 3, size = 3)+
scale_color_brewer(palette="Set1") +
scale_x_continuous(limits=c(0, X_MAX), breaks=seq(0, X_MAX, 500))+
scale_y_continuous(limits=c(0, Y_MAX), breaks=seq(0, Y_MAX, 500))+
theme_bw() +
theme(legend.position = "none", axis.title.x = element_blank(),
axis.title.y = element_blank())
ggsave(paste0("processed_data/calib_adjust/",
"/",s,".pdf"), width=8, height=4)
## now apply predictions to entire dataset and get out new x and y
adjusted_data <- lapply(subjs, function (s) { # for each subject in the dataset
out_data <- filter(d,subid == s)
models <- filter(models,subid == s)
out_data$x = predict(models$x_model[[1]], newdata = out_data)
out_data$y = predict(models$y_model[[1]], newdata = out_data)
return(out_data)}) %>%
bind_rows %>% # bind it all together
mutate(x = ifelse(x <= 0 | x >= X_MAX | y <= 0 | y >= Y_MAX, NA, x),
y = ifelse(x <= 0 | x >= X_MAX | y <= 0 | y >= Y_MAX, NA, y)) %>%
group_by(subid)
# 1. Filter data for word learning and test trials
toFilter=c('WordLearn','TestTrial')
d.cleaned <- d.cleaned %>%
filter(is.element(TrialType,toFilter))
d.cleaned <- adjusted_data
## between subjects variables not currently in pilot dataset: AuditoryCond,
##  data$stimGroup # stimuli grouping, between subjects variable
## data$AuditoryCond
d.cleaned$AuditoryCond ="Labels"
d.cleaned$StimGroup = "StimGroup1"
# Rename variables for clarity
d.cleaned$SimLevelDiscrete = d.cleaned$level
# d.cleaned$SimLevel = d.cleaned$similarity: will be pearson's correlation values between stimulus categories
d.cleaned$Participant = d.cleaned$subid
# 1. Filter data for word learning and test trials
toFilter=c('WordLearn','TestTrial')
d.cleaned <- d.cleaned %>%
filter(is.element(TrialType,toFilter))
# 2. Define the target ROIs (regions of interest)
rois <- list()
rois[[1]] <- c(0,200,768,600) # left
rois[[2]] <- c(1152,200,768,600) # right
names(rois) <- c("L","R")
roi.image(rois)
d.cleaned$roi <- roi.check(d.cleaned,rois) # calls helper function to say whether infant was looking at ROI or not
# 3. Code whether infant was looking in each ROI
d.cleaned <- d.cleaned %>%
mutate(novelty = ifelse(roi == distPos, "1", "0")) %>%
mutate(novelty = as.numeric(novelty))
# see how the distribution of ROIs looks
qplot(roi,data=d) # mostly looking in one or the other
d.cleaned$roi <- roi.check(d.cleaned,rois) # calls helper function to say whether infant was looking at ROI or not
d.cleaned$roi
d.cleaned <- d.cleaned %>%
mutate(novelty = ifelse(roi == distPos, "1", "0")) %>%
mutate(novelty = as.numeric(novelty))
# 1. Filter data for word learning and test trials
toFilter=c('WordLearn','TestTrial')
d.cleaned <- d.cleaned %>%
filter(is.element(TrialType,toFilter))
# 2. Define the target ROIs (regions of interest)
rois <- list()
rois[[1]] <- c(0,200,768,600) # left
rois[[2]] <- c(1152,200,768,600) # right
names(rois) <- c("L","R")
roi.image(rois)
d.cleaned$roi <- roi.check(d.cleaned,rois) # calls helper function to say whether infant was looking at ROI or not
# 3. Code whether infant was looking in each ROI
d.cleaned <- d.cleaned %>%
mutate(novelty = ifelse(roi == distPos, "1", "0")) %>%
mutate(novelty = as.numeric(novelty))
# see how the distribution of ROIs looks
qplot(roi,data=d.cleaned) # mostly looking in one or the other
# Rezero trials to onset of stimuli (test trial) or to onset of label (word learning trials)
d.cleaned <- rezero.trials(d.cleaned )
novPrefsByCond <- d.cleaned %>%
filter(TrialType == "TestTrial") %>%
group_by(Participant,AuditoryCond) %>%
dplyr::summarise(meanNovelty = mean(novelty, na.rm=TRUE))
LabelsNovelty = novPrefsByCond$meanNovelty[novPrefsByCond$AuditoryCond=="Labels"]
# TonesNovelty = novPrefsByCond$meanNovelty[novPrefsByCond$AuditoryCond=="Tones"]
# MusicNovelty = novPrefsByCond$meanNovelty[novPrefsByCond$AuditoryCond=="Music"]
# BayesFactor::ttestBF(LabelsNovelty, TonesNovelty)
# BayesFactor::ttestBF(LabelsNovelty, MusicNovelty)
novPrefsByCondbySim <- d.cleaned %>%
filter(TrialType == "TestTrial") %>%
group_by(Participant,AuditoryCond,SimLevel, StimGroup) %>%
dplyr::summarise(meanNovelty = mean(novelty, na.rm=TRUE))
novPrefsByCond <- d.cleaned %>%
filter(TrialType == "TestTrial") %>%
group_by(Participant,AuditoryCond) %>%
dplyr::summarise(meanNovelty = mean(novelty, na.rm=TRUE))
LabelsNovelty = novPrefsByCond$meanNovelty[novPrefsByCond$AuditoryCond=="Labels"]
# TonesNovelty = novPrefsByCond$meanNovelty[novPrefsByCond$AuditoryCond=="Tones"]
# MusicNovelty = novPrefsByCond$meanNovelty[novPrefsByCond$AuditoryCond=="Music"]
# BayesFactor::ttestBF(LabelsNovelty, TonesNovelty)
# BayesFactor::ttestBF(LabelsNovelty, MusicNovelty)
d.cleaned
d.cleaned$level
d.cleaned <- adjusted_data
## between subjects variables not currently in pilot dataset: AuditoryCond,
##  data$stimGroup # stimuli grouping, between subjects variable
## data$AuditoryCond
d.cleaned$AuditoryCond ="Labels"
d.cleaned$StimGroup = "StimGroup1"
d.cleaned$SimLevel = d$level
# Rename variables for clarity
d.cleaned$SimLevelDiscrete = d.cleaned$level
# d.cleaned$SimLevel = d.cleaned$similarity: will be pearson's correlation values between stimulus categories
d.cleaned$Participant = d.cleaned$subid
# 1. Filter data for word learning and test trials
toFilter=c('WordLearn','TestTrial')
d.cleaned <- d.cleaned %>%
filter(is.element(TrialType,toFilter))
# 2. Define the target ROIs (regions of interest)
rois <- list()
rois[[1]] <- c(0,200,768,600) # left
rois[[2]] <- c(1152,200,768,600) # right
names(rois) <- c("L","R")
roi.image(rois)
d.cleaned$roi <- roi.check(d.cleaned,rois) # calls helper function to say whether infant was looking at ROI or not
# 3. Code whether infant was looking in each ROI
d.cleaned <- d.cleaned %>%
mutate(novelty = ifelse(roi == distPos, "1", "0")) %>%
mutate(novelty = as.numeric(novelty))
# see how the distribution of ROIs looks
qplot(roi,data=d.cleaned) # mostly looking in one or the other
# Rezero trials to onset of stimuli (test trial) or to onset of label (word learning trials)
d.cleaned <- rezero.trials(d.cleaned )
novPrefsByCond <- d.cleaned %>%
filter(TrialType == "TestTrial") %>%
group_by(Participant,AuditoryCond) %>%
dplyr::summarise(meanNovelty = mean(novelty, na.rm=TRUE))
LabelsNovelty = novPrefsByCond$meanNovelty[novPrefsByCond$AuditoryCond=="Labels"]
# TonesNovelty = novPrefsByCond$meanNovelty[novPrefsByCond$AuditoryCond=="Tones"]
# MusicNovelty = novPrefsByCond$meanNovelty[novPrefsByCond$AuditoryCond=="Music"]
# BayesFactor::ttestBF(LabelsNovelty, TonesNovelty)
# BayesFactor::ttestBF(LabelsNovelty, MusicNovelty)
novPrefsByCondbySim <- d.cleaned %>%
filter(TrialType == "TestTrial") %>%
group_by(Participant,AuditoryCond,SimLevel, StimGroup) %>%
dplyr::summarise(meanNovelty = mean(novelty, na.rm=TRUE))
famPrefsByCondbySim <- d.cleaned %>%
mutate(familiarity = 1 - novelty) %>%
filter(TrialType == "WordLearn") %>%
group_by(Participant,AuditoryCond,SimLevel,StimGroup) %>%
dplyr::summarise(meanFamiliarity = mean(familiarity, na.rm=TRUE))
## Scale similarity for modeling purposes
novPrefsByCondbySimbyGroup$SimLevel = scale(novPrefsByCondbySimbyGroup$SimLevel)
novPrefsByCondbySimbyGroup <- d.cleaned %>%
filter(TrialType == "TestTrial") %>%
group_by(Participant,AuditoryCond,SimLevel, StimGroup) %>%
dplyr::summarise(meanNovelty = mean(novelty, na.rm=TRUE))
famPrefsByCondbySimbyGroup <- d.cleaned %>%
mutate(familiarity = 1 - novelty) %>%
filter(TrialType == "WordLearn") %>%
group_by(Participant,AuditoryCond,SimLevel,StimGroup) %>%
dplyr::summarise(meanFamiliarity = mean(familiarity, na.rm=TRUE))
## Scale similarity for modeling purposes
novPrefsByCondbySimbyGroup$SimLevel = scale(novPrefsByCondbySimbyGroup$SimLevel)
famPrefsByCondbySim$SimLevel = scale(famPrefsByCondbySim$SimLevel)
## H3.
MaximalModel_TestTrials <- brm(meanNovelty ~ AuditoryCond * SimLevel + (1 + AuditoryCond*SimLevel | Participant) + (1 + AuditoryCond*SimLevel | StimGroup), data=novPrefsByCondbySimbyGroup)
## H4
MaximalModel_WordLearn <- brm(meanFamiliarity ~ AuditoryCond * SimLevel + (1 + AuditoryCond*SimLevel | Participant) + (1 + AuditoryCond*SimLevel | famPrefsByCondbySimbyGroup))
## H3.
MaximalModel_TestTrials <- brm(meanNovelty ~ AuditoryCond * SimLevel + (1 + AuditoryCond*SimLevel | Participant) + (1 + AuditoryCond*SimLevel | StimGroup), data=novPrefsByCondbySimbyGroup)
## H4
MaximalModel_WordLearn <- brm(meanFamiliarity ~ AuditoryCond * SimLevel + (1 + AuditoryCond*SimLevel | Participant) + (1 + AuditoryCond*SimLevel | famPrefsByCondbySimbyGroup), data = famPrefsByCondbySimbyGroup)
## subsample the data so that you get smooth curves
subsample.hz <- 10 # 10 hz is decent, eventually we should set to 30 or 60 hz
d.cleaned$t.crit.binned <- round(d.cleaned$t.crit*subsample.hz)/subsample.hz # subsample step
## 1. Time courses: Trial Type  (test trial, word learning) x Auditory Cond (labels, tones, music)
ms <- aggregate(novelty ~ t.crit.binned + TrialType + AuditoryCond, d.cleaned, mean)
qplot(t.crit.binned, novelty,
colour=TrialType,
geom="point",
data=ms) +
geom_hline(yintercept=.5,lty=2) +
geom_hline(yintercept=.5,lty=4) +
geom_vline(xintercept=0,lty=3) +
geom_smooth(method="loess",span=.5) +
facet_wrap(~ TrialType + AuditoryCond) +
xlab("Time (s)") + ylab("Proportion looking at NOVEL Item") +
scale_x_continuous(limits=c(-1,10),expand = c(0,0)) +
scale_y_continuous(limits=c(0,1),expand = c(0,0)) # make the axes start at 0
## 1. Time courses: Trial Type  (test trial, word learning) x Auditory Cond (labels, tones, music) x Similarity (discretized)
ms <- aggregate(novelty ~ t.crit.binned + TrialType + SimLevelDiscrete + AuditoryCond, d.cleaned, mean)
qplot(t.crit.binned, novelty,
colour=level,
geom="point",
data=ms) +
geom_hline(yintercept=.5,lty=2) +
geom_hline(yintercept=.5,lty=4) +
geom_vline(xintercept=0,lty=3) +
geom_smooth(method="loess",span=1) +
facet_wrap(~ TrialType + SimLevelDiscrete + AuditoryCond) +
xlab("Time (s)") + ylab("Proportion looking at NOVEL item") +
scale_x_continuous(limits=c(-1,10),expand = c(0,0)) +
scale_y_continuous(limits=c(0,1),expand = c(0,0)) # make the axes start at 0
## subsample the data so that you get smooth curves
subsample.hz <- 10 # 10 hz is decent, eventually we should set to 30 or 60 hz
d.cleaned$t.crit.binned <- round(d.cleaned$t.crit*subsample.hz)/subsample.hz # subsample step
## 1. Time courses: Trial Type  (test trial, word learning) x Auditory Cond (labels, tones, music)
ms <- aggregate(novelty ~ t.crit.binned + TrialType + AuditoryCond, d.cleaned, mean)
qplot(t.crit.binned, novelty,
colour=TrialType,
geom="point",
data=ms) +
geom_hline(yintercept=.5,lty=2) +
geom_hline(yintercept=.5,lty=4) +
geom_vline(xintercept=0,lty=3) +
geom_smooth(method="loess",span=.5) +
facet_wrap(~ TrialType + AuditoryCond) +
xlab("Time (s)") + ylab("Proportion looking at NOVEL Item") +
scale_x_continuous(limits=c(-1,10),expand = c(0,0)) +
scale_y_continuous(limits=c(0,1),expand = c(0,0)) # make the axes start at 0
## 1. Time courses: Trial Type  (test trial, word learning) x Auditory Cond (labels, tones, music) x Similarity (discretized)
ms <- aggregate(novelty ~ t.crit.binned + TrialType + SimLevelDiscrete + AuditoryCond, d.cleaned, mean)
qplot(t.crit.binned, novelty,
colour=SimLevel,
geom="point",
data=ms) +
geom_hline(yintercept=.5,lty=2) +
geom_hline(yintercept=.5,lty=4) +
geom_vline(xintercept=0,lty=3) +
geom_smooth(method="loess",span=1) +
facet_wrap(~ TrialType + SimLevelDiscrete + AuditoryCond) +
xlab("Time (s)") + ylab("Proportion looking at NOVEL item") +
scale_x_continuous(limits=c(-1,10),expand = c(0,0)) +
scale_y_continuous(limits=c(0,1),expand = c(0,0)) # make the axes start at 0
d.cleaned$SimLevel
## subsample the data so that you get smooth curves
subsample.hz <- 10 # 10 hz is decent, eventually we should set to 30 or 60 hz
d.cleaned$t.crit.binned <- round(d.cleaned$t.crit*subsample.hz)/subsample.hz # subsample step
## 1. Time courses: Trial Type  (test trial, word learning) x Auditory Cond (labels, tones, music)
ms <- aggregate(novelty ~ t.crit.binned + TrialType + AuditoryCond, d.cleaned, mean)
qplot(t.crit.binned, novelty,
colour=TrialType,
geom="point",
data=ms) +
geom_hline(yintercept=.5,lty=2) +
geom_hline(yintercept=.5,lty=4) +
geom_vline(xintercept=0,lty=3) +
geom_smooth(method="loess",span=.5) +
facet_wrap(~ TrialType + AuditoryCond) +
xlab("Time (s)") + ylab("Proportion looking at NOVEL Item") +
scale_x_continuous(limits=c(-1,10),expand = c(0,0)) +
scale_y_continuous(limits=c(0,1),expand = c(0,0)) # make the axes start at 0
## 1. Time courses: Trial Type  (test trial, word learning) x Auditory Cond (labels, tones, music) x Similarity (discretized)
ms <- aggregate(novelty ~ t.crit.binned + TrialType + SimLevelDiscrete + AuditoryCond, d.cleaned, mean)
qplot(t.crit.binned, novelty,
colour=SimLevelDiscrete,
geom="point",
data=ms) +
geom_hline(yintercept=.5,lty=2) +
geom_hline(yintercept=.5,lty=4) +
geom_vline(xintercept=0,lty=3) +
geom_smooth(method="loess",span=1) +
facet_wrap(~ TrialType + SimLevelDiscrete + AuditoryCond) +
xlab("Time (s)") + ylab("Proportion looking at NOVEL item") +
scale_x_continuous(limits=c(-1,10),expand = c(0,0)) +
scale_y_continuous(limits=c(0,1),expand = c(0,0)) # make the axes start at 0
